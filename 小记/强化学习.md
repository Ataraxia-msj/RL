# 强化学习的基本流程

## 环境初始化：

智能体与环境进行交互。环境会反馈当前的状态。
## 智能体选择动作：

基于当前的策略，智能体选择一个动作。 
## 执行动作并观察奖励：

智能体执行动作后，环境会反馈一个奖励，并且智能体转移到下一个状态。  
## 更新策略：

智能体根据获取的奖励和下一个状态来调整策略，以期获得更高的长期回报。  
## 循环重复：

这一过程会持续进行，直到达到设定的停止条件（例如：达到最大回合数、目标达成等）。


# 马尔可夫决策过程

> 解决强化学习的一个框架
## 马尔科夫性

系统的下一个状态仅与当前的状态有关系，与之前的状态没有关系。
## 马尔科夫过程

马尔科夫过程是一个二元组（S,P），S是有限状态集合，P是状态转移概率。
## 马尔可夫决策过程

马尔科夫决策过程由元组（S,A,P,R,折扣因子）描述。其中：S是状态集，A是动作集，P是状态转移概率，R是回报函数。

与马尔科夫过程不同的是，马尔科夫决策过程的状态转移概率是包含动作。

强化学习的目标是给定一个马尔科夫决策过程，寻找最优策略。
## 策略

策略是指状态到动作的映射，用符号Π表示。是给定状态s时，动作集上的一个分布

> 我理解的就是，在状态s的情况下选择动作a的概率

最优策略就是最大化回报。

累计回报：

![[Pasted image 20241022214059.png]]

## 状态值函数和状态动作值函数：

因为策略的随机导致累计回报是随机的，为了找到一个标准衡量策略的优劣。

状态值函数：当智能体采用策略时，累计回报服从一个分布，累计回报在状态s处的期望值定义为状态值函数：

![[Pasted image 20241022215125.png]]

状态-动作值函数：就是添加了动作条件

![[Pasted image 20241022220249.png]]

### 贝尔曼方程


![[Pasted image 20241022221140.png]]

# 强化学习中常用的随机策略

## 1. 贪婪策略

只有在使得动作值函数最大的动作处取概率1，其他动作的概率为0.

![[Pasted image 20241023162539.png]]

## 2. 随机贪婪策略

增加了一定的随机概率。平衡了利用和探索

![[Pasted image 20241023162731.png]]

## 3. 高斯策略

高斯策略在连续系统的强化学习中应用广泛。


# 基于模型的动态规划


# 强化学习分类

![[Pasted image 20241023175448.png]]



# 强化学习基本概念
## 基本元素


- Agent（智能体）：就是那个不停运动的个体，是所有的载体。

- Envirotment（环境）：智能体与环境交互，每当智能体做出一个行动，环境会给智能体反馈一个奖励和智能体当前的状态。

- Goal（目标）：就是我们训练的目标。如果用强化学习的语言就是两类，最优策略或者最大奖励。
## 主要元素
> 从这里开始就要有一些专业性的设计了。一个合适的状态定义和动作定义，有利于训练出更好的智能体。奖励的设定是否合适会很影响最后的结果。

- State（状态）：一个智能体在环境中就会有各种各样的状态。

- Action（行动）：在任何一个状态下，智能体都会采取一些行动。

- Reward（奖励）：环境给智能体行动的反馈，这些奖励会用来评判行动的好坏。应该以目标为基础设置奖励。
## 核心元素

- Policy（策略）：一系列动作就是一个策略。我们要找的就是最优的策略。

- 状态转移函数：智能体在做出动作a之后，状态从s转移到s'的概率。

- Return（回报）：就是从t时刻开始，之后所有奖励的加权求和。他是一个随机变量，因为他取决于之后的奖励，之后的奖励由取决于未来的状态和所选择的动作。
	- 折扣因子：0-1的值，是一个超参数，需要我们自己定。他主要是决定未来的奖励更重要还是及时的奖励更重要。

- ==Value（价值）：==
	- 动作价值函数：就是对回报求期望，来消除它的随机性。

# DQN

用一个神经网络来求Q*。
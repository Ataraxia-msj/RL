# 初步测试

1. 虚拟10台老虎机

		import numpy as np
		#每个老虎机的中奖概率
		proobs = np.random.uniform(size = 10)
		#记录每个老虎的返回值
		rewards = [ [1] for_ in range(10)]
		probs,rewards

2. 动作函数

		import random
		#贪婪算法
		def choose_one()
			# 有小概率随机选择一根拉杆（探索）
			if random.random()<0.01:
				return random.randint(0,9)
			# 计算每个老虎机的平均奖励
			rewards_mean = [np.mean(i) for i in rewards]
			# 选择估值最大的拉杆
			return np.argmax(rewards_mean)

3. 训练

		def try_and_play():
			i = choose_one()
			reward = 0
			if randow.randow()<probs[i]:
				reward = 1
			rewards[i].append(reward)
			
		def get_result():
			#玩N次
			for _ in range(5000):
			 try_and_play()
			target = probs.max() * 5000
			result = sum([sum(i) for i in rewards])
			return target,result
			
# 改进

## 1. 递减的贪婪算法

探索欲望逐渐降低。在上面简单的贪婪算法中，探索的欲望是固定的。但是正常情况是，我积累的经验越多，我探索的就越少。

主要在动作函数里面修改：


	import random
	def choose_one()
		# 求已经完了多少次
		played_count = sum([len(i) for i in rewards])
		#次数越多，探索欲望越低
		if random.random()<1 / played_count:
			return random.randint(0,9)
		# 计算每个老虎机的平均奖励
		rewards_mean = [np.mean(i) for i in rewards]
		# 选择估值最大的拉杆
		return np.argmax(rewards_mean)

## 2. UCB算法，上置信界算法

多探索玩的少的。还是修改动作函数：

![[Pasted image 20241108154720.png]]


## 3. 汤普森采样

使用Beta分布采样，当数值特别大的时候，Beta分布会趋于稳定。前面在探索，后面趋于利用。

![[Pasted image 20241108154951.png]]

